### webcrawler: 웹 페이지의 데이터를 모아주는 소프트웨어
### webcrawling: 크롤러를 사용해 웹 페이지의 데이터를 추출해 내는 행위

#### 외부 모듈 설치(파이썬)
pip install requests

#### code 작성
import requests <br>
print(requests)
<br>
get 함수: return 응답값 ==> requests.get(url) 
<br>url이라는 것을 재료로 한다

#### get 요청을 보내는 기능(+put, post, delete도 있음)
요청 - 응답(response) <br>
requests.get(url,params=None,**kwargs)
<br>requests.get(url) -> return: requests.response

#### code
import requests
<br>url="https://www.youtube.com/"
<br>print(requests.get(url)) 
<br>결과값: <Response [200]>; 200은 성공을 의미
<br>원하는 결과값이 아님
<br>url="https://www.youtube.com/"
<br>response = requests.get(url) requests 모듈안에 있는 값을 채워서 응답
<br>print(response.text) -> 응답내용이 text 형태로 출력
<br>print(response.url) -> response에 담길 최종 url 값 출력
<br>print(response.content) -> 응답내용이 text 이외의 형태로 출력
<br>print(response.encoding) -> 인코딩 종류 출력
<br>print(response.headers) -> header 값 표시
<br>print(response.json) -> JSON 형태 출력
<br>print(response.links) -> 포함된 링크 출력
<br>print(response.ok) -> 성공/실패 여부 출력(T/F)
<br>print(response.status_code) -> 코드화된 상태로 성공/실패 여부 출력, 200이면 성공

#### beautifulSoup import
import requests
from bs4 import beautifulSoup <br>#bs4모듈에서 beautifulSoup 기능을 가져온다
<br>url = "https://www.youtube.com/"
response = requests.get(url)
<br>print(BeautifulSoup(response.text, 'html.parser')) => response.text와 똑같은 결과로 보이지만 다른 데이터
<br>type(response.text) = <class 'str'>
<br>type(BeautifulSoup(response.text, 'html.parser')) = <class 'bs4.BeautifulSoup'>

#### BeautifulSoup(데이터, 파싱방법)
데이터 - html, xml => requests 모듈을 이용해서 받아옴
<br>파싱방법 - html.parser
<br>=>BeautifulSoup(response.text, 'html.parser')
<br>soup = BeautifulSoup(response.text, 'html.parser')
<br>print(soup.title) => <title>Youtube</title>
<br>print(soup.title.string) => Youtube
<br>print(soup.span) => 가장 상단에 있는 span태그만 출력
<br>print(soup.findAll('span')) => span 태그를 모두 가져옴

#### file
<br>file = open("daum.html". "w")
<br>file.write(response.text)
<br>file.close()

#### html 문서에서 모든 a태그를 가져오는 코드
print(soup.findAll("a"))
print(soup.findAll("a","link_favorsch")) -> a태그 중 실시간검색어 태그 출력
results = soup.findAll("a","link_favorsch")

#### 실시간 검색어 예쁘게 출력하기
<br>for result in results: 
<br><t>print(result.get_text(), "\n")
<br>rank = 1 # 실시간 검색어 순위
<br>for result in results: 
<br><t>print(rank, "위 : ",result.get_text(), "\n")
<br><t>rank += 1
<br> from datetime import datetime => 실시간검색어 날짜
<br> print(datetime.today().strftime("%Y년 %m월 %d일의 실시간 검색어 순위입니다.\n")) -> for 문 앞에 작성

#### 파일로 출력하기
* open(파일, 모드)
open("rankresult.txt", 모드)
모드: r(=read), w(=write), a(=append)
###### code 작성
search_rank_file = open("rankresult.txt", "w")
search_rank_file.write(str(rank)+"위:"+result.get_text()+"\n")

#### 최종 코드
```python
from bs4 import BeautifulSoup
import requests
from datetime import datetime

headers = {"사용자 정보"} # 크롤링하는 사용자임을 인증하는 코드
url = "https://datalab.naver.com/keyword/realtimeList.naver?age=20s"
response = requests.get(url,headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')
rank = 1
results = soup.findAll('span','item_title')

print(response.text)

search_rank_file = open("rankresult.txt","a")

print(datetime.today().strftime("%Y년 %m월 %d일의 실시간 검색어 순위입니다.\n"))

for result in results:
    search_rank_file.write(str(rank)+"위:"+result.get_text()+"\n")
    print(rank,"위 : ",result.get_text(),"\n")
    rank += 1
```